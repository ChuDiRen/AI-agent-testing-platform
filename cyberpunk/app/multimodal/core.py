"""
å¤šæ¨¡æ€å†…å®¹å¤„ç†å™¨

æ”¯æŒå¤„ç†å¤šç§å†…å®¹ç±»å‹ï¼š
- ğŸ“„ PDFã€Wordã€PPTã€Excel æ–‡æ¡£
- ğŸ–¼ï¸ å›¾ç‰‡ã€å›¾è¡¨ã€æˆªå›¾
- ğŸ“Š è¡¨æ ¼ã€æ•°æ®ç»Ÿè®¡
- ğŸ”¢ æ•°å­¦å…¬å¼ï¼ˆLaTeXï¼‰

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ–‡æ¡£å†…å®¹æå–
2. å›¾ç‰‡å†…å®¹åˆ†æ
3. è¡¨æ ¼æ•°æ®å¤„ç†
4. æ•°å­¦å…¬å¼è§£æ
5. å†…å®¹å‘é‡åŒ–
6. è¯­ä¹‰æœç´¢
"""
import asyncio
import json
import base64
from typing import Dict, List, Any, Optional, Union
from pathlib import Path
from datetime import datetime
import hashlib
import re

from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain.document_loaders import (
#     PyPDFLoader, 
#     Docx2txtLoader, 
#     UnstructuredExcelLoader,
#     UnstructuredMarkdownLoader
# )
# from langchain.schema import Document


class MultimodalContent(BaseModel):
    """å¤šæ¨¡æ€å†…å®¹"""
    content_id: str = Field(description="å†…å®¹ID")
    content_type: str = Field(description="å†…å®¹ç±»å‹")
    file_path: Optional[str] = Field(default=None, description="æ–‡ä»¶è·¯å¾„")
    raw_content: str = Field(description="åŸå§‹å†…å®¹")
    processed_content: str = Field(description="å¤„ç†åå†…å®¹")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="å…ƒæ•°æ®")
    extracted_entities: List[Dict[str, Any]] = Field(default_factory=list, description="æå–çš„å®ä½“")
    relationships: List[Dict[str, Any]] = Field(default_factory=list, description="å…³ç³»")
    timestamp: str = Field(default_factory=lambda: datetime.utcnow().isoformat())


class ContentProcessor:
    """å¤šæ¨¡æ€å†…å®¹å¤„ç†å™¨"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model="deepseek-chat",
            temperature=0.3,
            base_url="https://api.siliconflow.cn/v1",
            api_key="YOUR_SILICONFLOW_API_KEY"
        )
        
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-ada-002",
            base_url="https://api.siliconflow.cn/v1",
            api_key="YOUR_SILICONFLOW_API_KEY"
        )
        
        # æ”¯æŒçš„å†…å®¹ç±»å‹å¤„ç†å™¨
        self.processors = {
            'pdf': self._process_pdf,
            'docx': self._process_docx,
            'pptx': self._process_pptx,
            'xlsx': self._process_xlsx,
            'md': self._process_markdown,
            'txt': self._process_text,
            'image': self._process_image,
            'chart': self._process_chart,
            'table': self._process_table,
            'formula': self._process_formula
        }
        
        # å†…å®¹æå–å™¨
        self.extractors = {
            'api_endpoints': self._extract_api_endpoints,
            'parameters': self._extract_parameters,
            'schemas': self._extract_schemas,
            'examples': self._extract_examples,
            'security': self._extract_security
        }
    
    async def process_content(self, file_path: str, content_type: str = None) -> MultimodalContent:
        """å¤„ç†å¤šæ¨¡æ€å†…å®¹"""
        content_id = hashlib.md5(file_path.encode()).hexdigest()
        
        # è‡ªåŠ¨æ£€æµ‹å†…å®¹ç±»å‹
        if not content_type:
            content_type = self._detect_content_type(file_path)
        
        # é€‰æ‹©å¤„ç†å™¨
        if content_type not in self.processors:
            raise ValueError(f"ä¸æ”¯æŒçš„å†…å®¹ç±»å‹: {content_type}")
        
        processor = self.processors[content_type]
        multimodal_content = await processor(file_path, content_id)
        
        # æå–APIç›¸å…³ä¿¡æ¯
        extracted_entities = await self._extract_api_entities(multimodal_content.raw_content)
        relationships = await self._extract_api_relationships(multimodal_content.raw_content)
        
        multimodal_content.extracted_entities = extracted_entities
        multimodal_content.relationships = relationships
        
        return multimodal_content
    
    async def _process_pdf(self, file_path: str, content_id: str) -> MultimodalContent:
        """å¤„ç†PDFæ–‡æ¡£"""
        try:
            # ä½¿ç”¨PyPDFLoaderæå–PDFå†…å®¹
            loader = PyPDFLoader(file_path)
            documents = loader.load()
            
            raw_content = "\n".join([doc.page_content for doc in documents])
            
            # å¤„ç†å†…å®¹
            processed_content = await self._clean_and_structure_content(raw_content)
            
            return MultimodalContent(
                content_id=content_id,
                content_type="pdf",
                file_path=file_path,
                raw_content=raw_content,
                processed_content=processed_content,
                metadata={
                    "pages": len(documents),
                    "extraction_method": "pypdf"
                }
            )
        except Exception as e:
            raise ValueError(f"PDFå¤„ç†å¤±è´¥: {str(e)}")
    
    async def _process_docx(self, file_path: str, content_id: str) -> MultimodalContent:
        """å¤„ç†Wordæ–‡æ¡£"""
        try:
            loader = Docx2txtLoader(file_path)
            documents = loader.load()
            
            raw_content = "\n".join([doc.page_content for doc in documents])
            
            # å¤„ç†å†…å®¹
            processed_content = await self._clean_and_structure_content(raw_content)
            
            return MultimodalContent(
                content_id=content_id,
                content_type="docx",
                file_path=file_path,
                raw_content=raw_content,
                processed_content=processed_content,
                metadata={
                    "documents": len(documents),
                    "extraction_method": "docx2txt"
                }
            )
        except Exception as e:
            raise ValueError(f"Wordæ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
    
    async def _process_pptx(self, file_path: str, content_id: str) -> MultimodalContent:
        """å¤„ç†PowerPointæ–‡æ¡£"""
        try:
            # æ¨¡æ‹ŸPPTå†…å®¹æå–
            raw_content = f"""
            PowerPointæ–‡æ¡£å†…å®¹: {file_path}
            å¹»ç¯ç‰‡1: APIæ¦‚è¿°
            - RESTful APIè®¾è®¡åŸåˆ™
            - HTTPçŠ¶æ€ç ä½¿ç”¨
            - è®¤è¯æœºåˆ¶ä»‹ç»
            
            å¹»ç¯ç‰‡2: æ¥å£è§„èŒƒ
            - ç”¨æˆ·ç®¡ç†æ¥å£
            - è®¤è¯æ¥å£
            - æ•°æ®æ¥å£
            
            å¹»ç¯ç‰‡3: æµ‹è¯•ç­–ç•¥
            - åŠŸèƒ½æµ‹è¯•
            - æ€§èƒ½æµ‹è¯•
            - å®‰å…¨æµ‹è¯•
            """
            
            processed_content = await self._clean_and_structure_content(raw_content)
            
            return MultimodalContent(
                content_id=content_id,
                content_type="pptx",
                file_path=file_path,
                raw_content=raw_content,
                processed_content=processed_content,
                metadata={
                    "slides": 3,
                    "extraction_method": "mock"
                }
            )
        except Exception as e:
            raise ValueError(f"PPTå¤„ç†å¤±è´¥: {str(e)}")
    
    async def _process_xlsx(self, file_path: str, content_id: str) -> MultimodalContent:
        """å¤„ç†Excelæ–‡æ¡£"""
        try:
            loader = UnstructuredExcelLoader(file_path)
            documents = loader.load()
            
            raw_content = "\n".join([doc.page_content for doc in documents])
            
            processed_content = await self._clean_and_structure_content(raw_content)
            
            return MultimodalContent(
                content_id=content_id,
                content_type="xlsx",
                file_path=file_path,
                raw_content=raw_content,
                processed_content=processed_content,
                metadata={
                    "sheets": len(documents),
                    "extraction_method": "unstructured"
                }
            )
        except Exception as e:
            raise ValueError(f"Excelå¤„ç†å¤±è´¥: {str(e)}")
    
    async def _process_markdown(self, file_path: str, content_id: str) -> MultimodalContent:
        """å¤„ç†Markdownæ–‡æ¡£"""
        try:
            loader = UnstructuredMarkdownLoader(file_path)
            documents = loader.load()
            
            raw_content = "\n".join([doc.page_content for doc in documents])
            
            processed_content = await self._clean_and_structure_content(raw_content)
            
            return MultimodalContent(
                content_id=content_id,
                content_type="md",
                file_path=file_path,
                raw_content=raw_content,
                processed_content=processed_content,
                metadata={
                    "documents": len(documents),
                    "extraction_method": "unstructured"
                }
            )
        except Exception as e:
            raise ValueError(f"Markdownå¤„ç†å¤±è´¥: {str(e)}")
    
    async def _process_text(self, file_path: str, content_id: str) -> MultimodalContent:
        """å¤„ç†çº¯æ–‡æœ¬æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                raw_content = f.read()
            
            processed_content = await self._clean_and_structure_content(raw_content)
            
            return MultimodalContent(
                content_id=content_id,
                content_type="txt",
                file_path=file_path,
                raw_content=raw_content,
                processed_content=processed_content,
                metadata={
                    "encoding": "utf-8",
                    "extraction_method": "file_read"
                }
            )
        except Exception as e:
            raise ValueError(f"æ–‡æœ¬å¤„ç†å¤±è´¥: {str(e)}")
    
    async def _process_image(self, file_path: str, content_id: str) -> MultimodalContent:
        """å¤„ç†å›¾ç‰‡æ–‡ä»¶"""
        try:
            # æ¨¡æ‹Ÿå›¾ç‰‡å†…å®¹åˆ†æ
            raw_content = f"""
            å›¾ç‰‡å†…å®¹åˆ†æ: {file_path}
            - APIæµç¨‹å›¾
            - æ¥å£æ¶æ„å›¾
            - æµ‹è¯•æµç¨‹å›¾
            - æ•°æ®æµå›¾
            """
            
            processed_content = await self._clean_and_structure_content(raw_content)
            
            return MultimodalContent(
                content_id=content_id,
                content_type="image",
                file_path=file_path,
                raw_content=raw_content,
                processed_content=processed_content,
                metadata={
                    "analysis_method": "mock_vision",
                    "content_types": ["diagram", "chart", "screenshot"]
                }
            )
        except Exception as e:
            raise ValueError(f"å›¾ç‰‡å¤„ç†å¤±è´¥: {str(e)}")
    
    async def _process_chart(self, file_path: str, content_id: str) -> MultimodalContent:
        """å¤„ç†å›¾è¡¨æ–‡ä»¶"""
        try:
            # æ¨¡æ‹Ÿå›¾è¡¨å†…å®¹åˆ†æ
            raw_content = f"""
            å›¾è¡¨å†…å®¹åˆ†æ: {file_path}
            - APIå“åº”æ—¶é—´ç»Ÿè®¡
            - æµ‹è¯•æˆåŠŸç‡å›¾è¡¨
            - æ€§èƒ½æŒ‡æ ‡å›¾è¡¨
            - é”™è¯¯åˆ†å¸ƒå›¾
            """
            
            processed_content = await self._clean_and_structure_content(raw_content)
            
            return MultimodalContent(
                content_id=content_id,
                content_type="chart",
                file_path=file_path,
                raw_content=raw_content,
                processed_content=processed_content,
                metadata={
                    "chart_types": ["line", "bar", "pie", "scatter"],
                    "analysis_method": "mock_chart"
                }
            )
        except Exception as e:
            raise ValueError(f"å›¾è¡¨å¤„ç†å¤±è´¥: {str(e)}")
    
    async def _process_table(self, file_path: str, content_id: str) -> MultimodalContent:
        """å¤„ç†è¡¨æ ¼æ•°æ®"""
        try:
            # æ¨¡æ‹Ÿè¡¨æ ¼æ•°æ®æå–
            raw_content = f"""
            è¡¨æ ¼æ•°æ®å†…å®¹: {file_path}
            APIç«¯ç‚¹åˆ—è¡¨:
            | ç«¯ç‚¹ | æ–¹æ³• | æè¿° | çŠ¶æ€ |
            |------|------|------|------|
            | /api/users | GET | è·å–ç”¨æˆ·åˆ—è¡¨ | æ´»è·ƒ |
            | /api/users | POST | åˆ›å»ºç”¨æˆ· | æ´»è·ƒ |
            | /api/auth | POST | ç”¨æˆ·è®¤è¯ | æ´»è·ƒ |
            
            æµ‹è¯•ç”¨ä¾‹æ•°æ®:
            | ç”¨ä¾‹ID | ç«¯ç‚¹ | çŠ¶æ€ | æ‰§è¡Œæ—¶é—´ |
            |--------|------|------|----------|
            | TC001 | /api/users | é€šè¿‡ | 245ms |
            | TC002 | /api/auth | é€šè¿‡ | 189ms |
            """
            
            processed_content = await self._clean_and_structure_content(raw_content)
            
            return MultimodalContent(
                content_id=content_id,
                content_type="table",
                file_path=file_path,
                raw_content=raw_content,
                processed_content=processed_content,
                metadata={
                    "tables": 2,
                    "extraction_method": "mock_table"
                }
            )
        except Exception as e:
            raise ValueError(f"è¡¨æ ¼å¤„ç†å¤±è´¥: {str(e)}")
    
    async def _process_formula(self, file_path: str, content_id: str) -> MultimodalContent:
        """å¤„ç†æ•°å­¦å…¬å¼"""
        try:
            # æ¨¡æ‹ŸLaTeXå…¬å¼æå–
            raw_content = f"""
            æ•°å­¦å…¬å¼å†…å®¹: {file_path}
            
            APIæ€§èƒ½è®¡ç®—å…¬å¼:
            $$T_{response} = T_{network} + T_{processing} + T_{database}$$
            
            æµ‹è¯•è¦†ç›–ç‡å…¬å¼:
            $$Coverage = \\frac{{Tested\ Cases}}{{Total\ Cases}} \\times 100\\%$$
            
            é”™è¯¯ç‡è®¡ç®—:
            $$Error\ Rate = \\frac{{Failed\ Tests}}{{Total\ Tests}} \\times 100\\%$$
            """
            
            processed_content = await self._clean_and_structure_content(raw_content)
            
            return MultimodalContent(
                content_id=content_id,
                content_type="formula",
                file_path=file_path,
                raw_content=raw_content,
                processed_content=processed_content,
                metadata={
                    "formulas": 3,
                    "extraction_method": "latex_mock"
                }
            )
        except Exception as e:
            raise ValueError(f"å…¬å¼å¤„ç†å¤±è´¥: {str(e)}")
    
    async def _clean_and_structure_content(self, raw_content: str) -> str:
        """æ¸…ç†å’Œç»“æ„åŒ–å†…å®¹"""
        # ä½¿ç”¨LLMæ¸…ç†å’Œç»“æ„åŒ–å†…å®¹
        prompt = f"""
        æ¸…ç†å’Œç»“æ„åŒ–ä»¥ä¸‹APIç›¸å…³å†…å®¹ï¼š
        
        åŸå§‹å†…å®¹ï¼š
        {raw_content}
        
        è¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
        1. ç§»é™¤æ— å…³å†…å®¹
        2. æå–APIç›¸å…³ä¿¡æ¯
        3. ç»“æ„åŒ–å†…å®¹
        4. æ·»åŠ é€‚å½“çš„æ ‡è®°
        
        è¿”å›æ¸…ç†åçš„ç»“æ„åŒ–å†…å®¹ã€‚
        """
        
        response = await self.llm.ainvoke(prompt)
        return response.content
    
    def _detect_content_type(self, file_path: str) -> str:
        """è‡ªåŠ¨æ£€æµ‹å†…å®¹ç±»å‹"""
        file_extension = Path(file_path).suffix.lower()
        
        type_mapping = {
            '.pdf': 'pdf',
            '.docx': 'docx',
            '.pptx': 'pptx',
            '.xlsx': 'xlsx',
            '.md': 'md',
            '.txt': 'txt',
            '.png': 'image',
            '.jpg': 'image',
            '.jpeg': 'image',
            '.gif': 'image',
            '.bmp': 'image',
            '.tiff': 'image',
            '.svg': 'chart',
            '.csv': 'table',
            '.tex': 'formula'
        }
        
        return type_mapping.get(file_extension, 'unknown')
    
    async def _extract_api_entities(self, content: str) -> List[Dict[str, Any]]:
        """æå–APIå®ä½“"""
        prompt = f"""
        ä»ä»¥ä¸‹å†…å®¹ä¸­æå–APIç›¸å…³çš„å®ä½“ä¿¡æ¯ï¼š
        
        å†…å®¹ï¼š
        {content}
        
        è¯·æå–ä»¥ä¸‹ç±»å‹çš„å®ä½“ï¼š
        1. APIç«¯ç‚¹ï¼ˆè·¯å¾„ã€æ–¹æ³•ï¼‰
        2. å‚æ•°ï¼ˆåç§°ã€ç±»å‹ã€ä½ç½®ï¼‰
        3. å“åº”å­—æ®µ
        4. è®¤è¯æ–¹å¼
        5. æ•°æ®æ¨¡å¼
        
        è¿”å›JSONæ ¼å¼ï¼š
        {{
            "entities": [
                {{
                    "type": "api_endpoint",
                    "name": "ç«¯ç‚¹åç§°",
                    "value": "ç«¯ç‚¹å€¼",
                    "confidence": 0.9
                }}
            ]
        }}
        """
        
        response = await self.llm.ainvoke(prompt)
        
        try:
            data = json.loads(response.content)
            return data.get("entities", [])
        except:
            return []
    
    async def _extract_api_relationships(self, content: str) -> List[Dict[str, Any]]:
        """æå–APIå…³ç³»"""
        prompt = f"""
        ä»ä»¥ä¸‹å†…å®¹ä¸­æå–APIç›¸å…³çš„å…³ç³»ä¿¡æ¯ï¼š
        
        å†…å®¹ï¼š
        {content}
        
        è¯·æå–ä»¥ä¸‹ç±»å‹çš„å…³ç³»ï¼š
        1. ç«¯ç‚¹ä¾èµ–å…³ç³»
        2. å‚æ•°å…³ç³»
        3. è®¤è¯ä¾èµ–
        4. æ•°æ®æµå…³ç³»
        
        è¿”å›JSONæ ¼å¼ï¼š
        {{
            "relationships": [
                {{
                    "source": "æºå®ä½“",
                    "target": "ç›®æ ‡å®ä½“",
                    "relationship_type": "å…³ç³»ç±»å‹",
                    "confidence": 0.9
                }}
            ]
        }}
        """
        
        response = await self.llm.ainvoke(prompt)
        
        try:
            data = json.loads(response.content)
            return data.get("relationships", [])
        except:
            return []
    
    async def _extract_api_endpoints(self, content: str) -> List[Dict[str, Any]]:
        """æå–APIç«¯ç‚¹"""
        # æå–APIç«¯ç‚¹ä¿¡æ¯
        endpoints = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ç«¯ç‚¹
        endpoint_patterns = [
            r'(GET|POST|PUT|DELETE|PATCH)\s+([/\w\-\{\}]+)',
            r'path:\s*([/\w\-\{\}]+)',
            r'endpoint:\s*([/\w\-\{\}]+)'
        ]
        
        for pattern in endpoint_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if len(match) == 2:
                    method, path = match
                    endpoints.append({
                        "type": "api_endpoint",
                        "method": method.upper(),
                        "path": path,
                        "confidence": 0.8
                    })
        
        return endpoints
    
    async def _extract_parameters(self, content: str) -> List[Dict[str, Any]]:
        """æå–å‚æ•°ä¿¡æ¯"""
        # æå–å‚æ•°ä¿¡æ¯
        parameters = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å‚æ•°
        param_patterns = [
            r'param:\s*([\w\-]+)',
            r'parameter:\s*([\w\-]+)',
            r'([\w\-]+):\s*([\w\-]+)'
        ]
        
        for pattern in param_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if len(match) == 1:
                    param_name = match[0]
                    parameters.append({
                        "type": "parameter",
                        "name": param_name,
                        "confidence": 0.7
                    })
                elif len(match) == 2:
                    param_name, param_type = match
                    parameters.append({
                        "type": "parameter",
                        "name": param_name,
                        "data_type": param_type,
                        "confidence": 0.8
                    })
        
        return parameters
    
    async def _extract_schemas(self, content: str) -> List[Dict[str, Any]]:
        """æå–æ•°æ®æ¨¡å¼"""
        # æå–æ•°æ®æ¨¡å¼ä¿¡æ¯
        schemas = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ¨¡å¼
        schema_patterns = [
            r'schema:\s*([\w\-]+)',
            r'model:\s*([\w\-]+)',
            r'structure:\s*([\w\-]+)'
        ]
        
        for pattern in schema_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                schemas.append({
                    "type": "schema",
                    "name": match,
                    "confidence": 0.6
                })
        
        return schemas
    
    async def _extract_examples(self, content: str) -> List[Dict[str, Any]]:
        """æå–ç¤ºä¾‹ä¿¡æ¯"""
        # æå–ç¤ºä¾‹ä¿¡æ¯
        examples = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ç¤ºä¾‹
        example_patterns = [
            r'example:\s*([\w\-]+)',
            r'sample:\s*([\w\-]+)',
            r'demo:\s*([\w\-]+)'
        ]
        
        for pattern in example_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                examples.append({
                    "type": "example",
                    "name": match,
                    "confidence": 0.5
                })
        
        return examples
    
    async def _extract_security(self, content: str) -> List[Dict[str, Any]]:
        """æå–å®‰å…¨ä¿¡æ¯"""
        # æå–å®‰å…¨ä¿¡æ¯
        security = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å®‰å…¨ä¿¡æ¯
        security_patterns = [
            r'auth:\s*([\w\-]+)',
            r'authentication:\s*([\w\-]+)',
            r'security:\s*([\w\-]+)',
            r'token:\s*([\w\-]+)'
        ]
        
        for pattern in security_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                security.append({
                    "type": "security",
                    "name": match,
                    "confidence": 0.7
                })
        
        return security
