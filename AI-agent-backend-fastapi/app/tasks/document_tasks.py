# Copyright (c) 2025 å·¦å²š. All rights reserved.
"""æ–‡æ¡£å¤„ç†å¼‚æ­¥ä»»åŠ¡"""
import os
import time
from datetime import datetime
from typing import Dict, Any
from celery import Task
from sqlalchemy import create_engine, select, update
from sqlalchemy.orm import sessionmaker

from app.core.celery_app import celery_app
from app.core.config import settings
from app.models.knowledge import Document, DocumentChunk, KnowledgeBase
from app.services.document_processor import document_processor
from app.services.vector_store import vector_store


# åˆ›å»ºåŒæ­¥æ•°æ®åº“ä¼šè¯(Celery workerä¸­ä½¿ç”¨)
sync_engine = create_engine(
    settings.DATABASE_URL.replace("sqlite+aiosqlite", "sqlite"),
    echo=False
)
SyncSessionLocal = sessionmaker(bind=sync_engine)


class DocumentProcessTask(Task):
    """æ–‡æ¡£å¤„ç†ä»»åŠ¡åŸºç±»"""
    
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        """ä»»åŠ¡å¤±è´¥å›è°ƒ"""
        print(f"âŒ ä»»åŠ¡å¤±è´¥: {task_id}, é”™è¯¯: {exc}")
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå¤±è´¥
        doc_id = args[0] if args else kwargs.get('doc_id')
        if doc_id:
            with SyncSessionLocal() as db:
                stmt = (
                    update(Document)
                    .where(Document.doc_id == doc_id)
                    .values(
                        status="error",
                        error_message=str(exc),
                        updated_at=datetime.now()
                    )
                )
                db.execute(stmt)
                db.commit()
    
    def on_success(self, retval, task_id, args, kwargs):
        """ä»»åŠ¡æˆåŠŸå›è°ƒ"""
        print(f"âœ… ä»»åŠ¡æˆåŠŸ: {task_id}")


@celery_app.task(
    bind=True,
    base=DocumentProcessTask,
    name="app.tasks.document_tasks.process_document_async",
    max_retries=3,
    default_retry_delay=60
)
def process_document_async(self, doc_id: int) -> Dict[str, Any]:
    """
    å¼‚æ­¥å¤„ç†æ–‡æ¡£ä»»åŠ¡
    
    Args:
        doc_id: æ–‡æ¡£ID
        
    Returns:
        å¤„ç†ç»“æœ
    """
    start_time = time.time()
    
    try:
        with SyncSessionLocal() as db:
            # è·å–æ–‡æ¡£
            doc = db.execute(
                select(Document).where(Document.doc_id == doc_id)
            ).scalar_one_or_none()
            
            if not doc:
                raise ValueError(f"æ–‡æ¡£ä¸å­˜åœ¨: {doc_id}")
            
            # è·å–çŸ¥è¯†åº“é…ç½®
            kb = db.execute(
                select(KnowledgeBase).where(KnowledgeBase.kb_id == doc.kb_id)
            ).scalar_one_or_none()
            
            if not kb:
                raise ValueError(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {doc.kb_id}")
            
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            self.update_state(
                state='PROCESSING',
                meta={'current': 0, 'total': 100, 'status': 'æ­£åœ¨è§£ææ–‡æ¡£...'}
            )
            
            doc.status = "processing"
            db.commit()
            
            # è§£ææ–‡æ¡£
            if doc.file_path and os.path.exists(doc.file_path):
                chunks, metadata = document_processor.process_document(
                    doc.file_path,
                    doc.file_type,
                    kb.chunk_size,
                    kb.chunk_overlap
                )
                doc.file_size = os.path.getsize(doc.file_path)
            elif doc.content:
                # ç›´æ¥åˆ†å—å†…å®¹
                chunks = document_processor.split_text(
                    doc.content,
                    kb.chunk_size,
                    kb.chunk_overlap
                )
                metadata = {"source": "direct_content"}
                doc.file_size = len(doc.content)
            else:
                raise ValueError("æ–‡æ¡£æ— å†…å®¹")
            
            # æ›´æ–°è¿›åº¦
            self.update_state(
                state='PROCESSING',
                meta={'current': 30, 'total': 100, 'status': f'å·²è§£æ,å…±{len(chunks)}ä¸ªåˆ†å—'}
            )
            
            # åˆ›å»ºåˆ†å—è®°å½•
            chunk_records = []
            for i, chunk_text in enumerate(chunks):
                chunk = DocumentChunk(
                    doc_id=doc.doc_id,
                    kb_id=doc.kb_id,
                    content=chunk_text,
                    chunk_index=i,
                    char_count=len(chunk_text),
                    metadata={"doc_name": doc.name, **metadata}
                )
                chunk_records.append(chunk)
                db.add(chunk)
            
            db.commit()
            
            # åˆ·æ–°è·å–chunk_id
            for chunk in chunk_records:
                db.refresh(chunk)
            
            # æ›´æ–°è¿›åº¦
            self.update_state(
                state='PROCESSING',
                meta={'current': 50, 'total': 100, 'status': 'æ­£åœ¨å‘é‡åŒ–...'}
            )
            
            # å‘é‡åŒ–å¹¶å­˜å‚¨
            collection_name = f"kb_{doc.kb_id}"
            texts = [chunk.content for chunk in chunk_records]
            metadatas = [
                {
                    "chunk_id": chunk.chunk_id,
                    "doc_id": chunk.doc_id,
                    "kb_id": chunk.kb_id,
                    "chunk_index": chunk.chunk_index,
                    "doc_name": doc.name
                }
                for chunk in chunk_records
            ]
            
            # æ‰¹é‡å‘é‡åŒ–(æ˜¾ç¤ºè¿›åº¦)
            batch_size = 10
            vector_ids = []
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                batch_metadatas = metadatas[i:i + batch_size]
                
                batch_vector_ids = vector_store.add_documents(
                    collection_name,
                    batch_texts,
                    batch_metadatas
                )
                vector_ids.extend(batch_vector_ids)
                
                # æ›´æ–°è¿›åº¦
                progress = 50 + int((i + batch_size) / len(texts) * 40)
                self.update_state(
                    state='PROCESSING',
                    meta={
                        'current': min(progress, 90),
                        'total': 100,
                        'status': f'å‘é‡åŒ–è¿›åº¦: {i + batch_size}/{len(texts)}'
                    }
                )
            
            # æ›´æ–°å‘é‡ID
            for chunk, vector_id in zip(chunk_records, vector_ids):
                chunk.vector_id = vector_id
            
            # æ›´æ–°æ–‡æ¡£å’ŒçŸ¥è¯†åº“ç»Ÿè®¡
            doc.status = "completed"
            doc.chunk_count = len(chunks)
            doc.char_count = sum(len(c) for c in chunks)
            doc.processed_at = datetime.now()
            
            kb.document_count += 1
            kb.chunk_count += len(chunks)
            kb.total_size += doc.file_size
            
            db.commit()
            
            # è®¡ç®—è€—æ—¶
            elapsed_time = time.time() - start_time
            
            return {
                "success": True,
                "doc_id": doc_id,
                "chunk_count": len(chunks),
                "elapsed_time": elapsed_time,
                "message": f"æ–‡æ¡£å¤„ç†æˆåŠŸ,å…±{len(chunks)}ä¸ªåˆ†å—"
            }
            
    except Exception as e:
        # è®°å½•é”™è¯¯
        print(f"âŒ å¤„ç†æ–‡æ¡£å¤±è´¥: {e}")
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€
        with SyncSessionLocal() as db:
            stmt = (
                update(Document)
                .where(Document.doc_id == doc_id)
                .values(
                    status="error",
                    error_message=str(e),
                    updated_at=datetime.now()
                )
            )
            db.execute(stmt)
            db.commit()
        
        # é‡è¯•
        raise self.retry(exc=e, countdown=60)


@celery_app.task(name="app.tasks.document_tasks.cleanup_expired_results")
def cleanup_expired_results():
    """æ¸…ç†è¿‡æœŸçš„ä»»åŠ¡ç»“æœ"""
    # TODO: å®ç°æ¸…ç†é€»è¾‘
    print("ğŸ§¹ æ¸…ç†è¿‡æœŸä»»åŠ¡ç»“æœ...")
    return {"success": True, "message": "æ¸…ç†å®Œæˆ"}


@celery_app.task(name="app.tasks.document_tasks.batch_process_documents")
def batch_process_documents(doc_ids: list) -> Dict[str, Any]:
    """
    æ‰¹é‡å¤„ç†æ–‡æ¡£
    
    Args:
        doc_ids: æ–‡æ¡£IDåˆ—è¡¨
        
    Returns:
        å¤„ç†ç»“æœ
    """
    results = []
    for doc_id in doc_ids:
        try:
            result = process_document_async.delay(doc_id)
            results.append({
                "doc_id": doc_id,
                "task_id": result.id,
                "status": "submitted"
            })
        except Exception as e:
            results.append({
                "doc_id": doc_id,
                "status": "failed",
                "error": str(e)
            })
    
    return {
        "success": True,
        "total": len(doc_ids),
        "results": results
    }

