"""
Agentic RAG ç³»ç»Ÿï¼ˆåŸºäº LangGraph çš„æ™ºèƒ½æ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰

è¿™ä¸ªæ–‡ä»¶å±•ç¤ºäº†å¦‚ä½•æ„å»ºä¸€ä¸ªå®Œæ•´çš„ Agentic RAG ç³»ç»Ÿï¼Œå®ƒå…·å¤‡ä»¥ä¸‹æ™ºèƒ½èƒ½åŠ›ï¼š
1. è‡ªä¸»å†³ç­–æ˜¯å¦éœ€è¦æ£€ç´¢ä¿¡æ¯
2. è¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯å¦ç›¸å…³
3. å¦‚æœæ–‡æ¡£ä¸ç›¸å…³ï¼Œè‡ªåŠ¨é‡å†™é—®é¢˜å¹¶é‡æ–°æ£€ç´¢
4. åŸºäºç›¸å…³æ–‡æ¡£ç”Ÿæˆå‡†ç¡®çš„ç­”æ¡ˆ

å·¥ä½œæµç¨‹ï¼š
ç”¨æˆ·æé—® â†’ å†³ç­–æ˜¯å¦æ£€ç´¢ â†’ æ£€ç´¢æ–‡æ¡£ â†’ è¯„ä¼°ç›¸å…³æ€§ â†’ ç”Ÿæˆç­”æ¡ˆ/é‡å†™é—®é¢˜
"""

import os
import sys
from typing import Literal

from pydantic import BaseModel, Field

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥è‡ªå®šä¹‰å·¥å…·
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils import load_chat_model  # ä½¿ç”¨è‡ªå®šä¹‰çš„load_chat_modelï¼ˆæ”¯æŒç¡…åŸºæµåŠ¨ï¼‰
from langchain_core.embeddings import DeterministicFakeEmbedding
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.tools import tool
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.prebuilt import ToolNode, tools_condition
from langchain_core.messages import HumanMessage

# ============ ç¯å¢ƒé…ç½® ============

# è®¾ç½® DeepSeek API å¯†é’¥
os.environ["SILICONFLOW_API_KEY"] = "sk-rmcrubplntqwdjumperktjbnepklekynmnmianaxtkneocem"

# åˆå§‹åŒ– DeepSeek èŠå¤©æ¨¡å‹
response_model = load_chat_model("siliconflow:deepseek-ai/DeepSeek-V3.2-Exp", temperature=0)
grader_model = load_chat_model("siliconflow:deepseek-ai/DeepSeek-V3.2-Exp", temperature=0)


# ============ ç¬¬ä¸€æ­¥ï¼šé¢„å¤„ç†æ–‡æ¡£ ============
# ä»ç½‘é¡µåŠ è½½æ–‡æ¡£å¹¶åˆ†å‰²æˆå°å—

print("ğŸ“š æ­£åœ¨åŠ è½½æ–‡æ¡£...")

# å®šä¹‰è¦åŠ è½½çš„åšå®¢æ–‡ç« URLåˆ—è¡¨
urls = [
    "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/",
    "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/",
]

# ä»æ¯ä¸ªURLåŠ è½½æ–‡æ¡£
docs = [WebBaseLoader(url).load() for url in urls]

# å°†åµŒå¥—åˆ—è¡¨å±•å¹³ä¸ºå•ä¸€åˆ—è¡¨
docs_list = [item for sublist in docs for item in sublist]

# åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨ï¼Œå°†é•¿æ–‡æ¡£åˆ†å‰²æˆå°å—
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=500,      # æ¯ä¸ªæ–‡æœ¬å—çš„å¤§å°ï¼ˆtokenæ•°ï¼‰
    chunk_overlap=100    # æ–‡æœ¬å—ä¹‹é—´çš„é‡å éƒ¨åˆ†ï¼ˆtokenæ•°ï¼‰
)

# åˆ†å‰²æ–‡æ¡£
doc_splits = text_splitter.split_documents(docs_list)
print(f"âœ… æ–‡æ¡£å·²åˆ†å‰²æˆ {len(doc_splits)} ä¸ªå°å—")


# ============ ç¬¬äºŒæ­¥ï¼šåˆ›å»ºæ£€ç´¢å·¥å…· ============
# ä½¿ç”¨å‘é‡å­˜å‚¨å’ŒåµŒå…¥æ¨¡å‹åˆ›å»ºæ£€ç´¢å™¨

print("ğŸ”§ æ­£åœ¨åˆ›å»ºæ£€ç´¢å·¥å…·...")

# åˆ›å»ºåµŒå…¥æ¨¡å‹ï¼ˆç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼‰
embeddings = DeterministicFakeEmbedding(size=4096)

# åˆ›å»ºå‘é‡å­˜å‚¨å¹¶æ·»åŠ æ–‡æ¡£
vectorstore = InMemoryVectorStore.from_documents(
    documents=doc_splits,
    embedding=embeddings
)

# åˆ›å»ºæ£€ç´¢å™¨
retriever = vectorstore.as_retriever(k=3)  # æ¯æ¬¡æ£€ç´¢è¿”å›3ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£

# åˆ›å»ºæ£€ç´¢å·¥å…·
@tool
def retriever_tool(query: str) -> str:
    """æœç´¢å¹¶è¿”å›å…³äº Lilian Weng åšå®¢æ–‡ç« çš„ä¿¡æ¯ã€‚å½“éœ€è¦å›ç­”å…³äº AIã€LLMã€æç¤ºå·¥ç¨‹ã€ä»£ç†ç­‰ä¸»é¢˜çš„é—®é¢˜æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚"""
    docs = retriever.invoke(query)
    return "\n\n".join(doc.page_content for doc in docs)

print("âœ… æ£€ç´¢å·¥å…·åˆ›å»ºå®Œæˆ")


# ============ ç¬¬ä¸‰æ­¥ï¼šç”ŸæˆæŸ¥è¯¢æˆ–å“åº”èŠ‚ç‚¹ ============
# å†³å®šæ˜¯å¦éœ€è¦æ£€ç´¢ä¿¡æ¯

def generate_query_or_respond(state: MessagesState):
    """
    æ ¹æ®å½“å‰çŠ¶æ€è°ƒç”¨æ¨¡å‹ç”Ÿæˆå“åº”ã€‚
    
    ç»™å®šé—®é¢˜åï¼Œæ¨¡å‹ä¼šå†³å®šï¼š
    1. ä½¿ç”¨æ£€ç´¢å·¥å…·è·å–ä¿¡æ¯
    2. ç›´æ¥å›ç­”ç”¨æˆ·ï¼ˆå¦‚æœä¸éœ€è¦é¢å¤–ä¿¡æ¯ï¼‰
    
    å‚æ•°:
        state: åŒ…å«æ¶ˆæ¯å†å²çš„çŠ¶æ€å¯¹è±¡
        
    è¿”å›:
        åŒ…å«æ¨¡å‹å“åº”çš„å­—å…¸
    """
    response = (
        response_model
        .bind_tools([retriever_tool])  # ç»‘å®šæ£€ç´¢å·¥å…·
        .invoke(state["messages"])
    )
    return {"messages": [response]}


# ============ ç¬¬å››æ­¥ï¼šè¯„ä¼°æ–‡æ¡£ç›¸å…³æ€§ ============
# åˆ¤æ–­æ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯å¦ä¸é—®é¢˜ç›¸å…³

# å®šä¹‰è¯„ä¼°æç¤ºè¯
GRADE_PROMPT = (
    "ä½ æ˜¯ä¸€ä¸ªè¯„ä¼°æ£€ç´¢æ–‡æ¡£ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³æ€§çš„è¯„åˆ†å‘˜ã€‚\n"
    "è¿™æ˜¯æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼š\n\n{context}\n\n"
    "è¿™æ˜¯ç”¨æˆ·çš„é—®é¢˜ï¼š{question}\n"
    "å¦‚æœæ–‡æ¡£åŒ…å«ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„å…³é”®è¯æˆ–è¯­ä¹‰å«ä¹‰ï¼Œåˆ™å°†å…¶è¯„ä¸ºç›¸å…³ã€‚\n"
    "ç»™å‡ºäºŒå…ƒè¯„åˆ† 'yes' æˆ– 'no'ï¼Œè¡¨ç¤ºæ–‡æ¡£æ˜¯å¦ä¸é—®é¢˜ç›¸å…³ã€‚"
)

# å®šä¹‰è¯„åˆ†æ•°æ®æ¨¡å‹
class GradeDocuments(BaseModel):
    """ä½¿ç”¨äºŒå…ƒè¯„åˆ†æ£€æŸ¥æ–‡æ¡£ç›¸å…³æ€§ã€‚"""
    reasoning: str = Field(
        description="è¯„ä¼°æ–‡æ¡£ç›¸å…³æ€§çš„æ¨ç†è¿‡ç¨‹"
    )
    answer: str = Field(
        description="ç›¸å…³æ€§è¯„åˆ†ï¼š'yes' è¡¨ç¤ºç›¸å…³ï¼Œ'no' è¡¨ç¤ºä¸ç›¸å…³"
    )

def grade_documents(
    state: MessagesState,
) -> Literal["generate_answer", "rewrite_question"]:
    """
    åˆ¤æ–­æ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯å¦ä¸é—®é¢˜ç›¸å…³ã€‚
    
    å‚æ•°:
        state: åŒ…å«æ¶ˆæ¯å†å²çš„çŠ¶æ€å¯¹è±¡
        
    è¿”å›:
        ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„åç§°ï¼š
        - "generate_answer": å¦‚æœæ–‡æ¡£ç›¸å…³
        - "rewrite_question": å¦‚æœæ–‡æ¡£ä¸ç›¸å…³
    """
    # è·å–åŸå§‹é—®é¢˜å’Œæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
    question = state["messages"][0].content
    context = state["messages"][-1].content
    
    # æ„å»ºè¯„ä¼°æç¤º
    prompt = GRADE_PROMPT.format(question=question, context=context)
    
    try:
        # è°ƒç”¨æ¨¡å‹è¿›è¡Œè¯„åˆ†
        response = (
            grader_model
            .with_structured_output(GradeDocuments)
            .invoke([{"role": "user", "content": prompt}])
        )
        
        score = response.answer
        
    except Exception as e:
        print(f"âš ï¸ ç»“æ„åŒ–è¾“å‡ºå¤±è´¥ï¼Œå°è¯•è§£ææ–‡æœ¬å“åº”: {e}")
        # å¦‚æœç»“æ„åŒ–è¾“å‡ºå¤±è´¥ï¼Œå°è¯•è·å–æ–‡æœ¬å“åº”å¹¶æ‰‹åŠ¨è§£æ
        text_response = grader_model.invoke([{"role": "user", "content": prompt}])
        
        # ç®€å•çš„æ–‡æœ¬è§£ææ¥æå– yes/no
        content = text_response.content.lower()
        if "yes" in content or "ç›¸å…³" in content:
            score = "yes"
        else:
            score = "no"
    
    if score == "yes":
        print("âœ… æ–‡æ¡£ç›¸å…³ï¼Œç»§ç»­ç”Ÿæˆç­”æ¡ˆ")
        return "generate_answer"
    else:
        print("âš ï¸ æ–‡æ¡£ä¸ç›¸å…³ï¼Œé‡å†™é—®é¢˜")
        return "rewrite_question"


# ============ ç¬¬äº”æ­¥ï¼šé‡å†™é—®é¢˜èŠ‚ç‚¹ ============
# å¦‚æœæ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸ç›¸å…³ï¼Œé‡å†™é—®é¢˜ä»¥æ”¹è¿›æ£€ç´¢æ•ˆæœ

# å®šä¹‰é‡å†™æç¤ºè¯
REWRITE_PROMPT = (
    "æŸ¥çœ‹è¾“å…¥å¹¶å°è¯•æ¨ç†å…¶æ½œåœ¨çš„è¯­ä¹‰æ„å›¾/å«ä¹‰ã€‚\n"
    "è¿™æ˜¯åˆå§‹é—®é¢˜ï¼š\n"
    "------- \n"
    "{question}\n"
    "------- \n"
    "åˆ¶å®šä¸€ä¸ªæ”¹è¿›çš„é—®é¢˜ï¼š"
)

def rewrite_question(state: MessagesState):
    """
    é‡å†™åŸå§‹ç”¨æˆ·é—®é¢˜ä»¥æ”¹è¿›æ£€ç´¢æ•ˆæœã€‚
    
    å‚æ•°:
        state: åŒ…å«æ¶ˆæ¯å†å²çš„çŠ¶æ€å¯¹è±¡
        
    è¿”å›:
        åŒ…å«é‡å†™åé—®é¢˜çš„å­—å…¸
    """
    messages = state["messages"]
    question = messages[0].content
    
    # æ„å»ºé‡å†™æç¤º
    prompt = REWRITE_PROMPT.format(question=question)
    
    # è°ƒç”¨æ¨¡å‹é‡å†™é—®é¢˜
    response = response_model.invoke([{"role": "user", "content": prompt}])
    
    print(f"ğŸ”„ é—®é¢˜å·²é‡å†™: {response.content}")
    
    return {"messages": [HumanMessage(content=response.content)]}


# ============ ç¬¬å…­æ­¥ï¼šç”Ÿæˆç­”æ¡ˆèŠ‚ç‚¹ ============
# åŸºäºæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ

# å®šä¹‰ç”Ÿæˆç­”æ¡ˆçš„æç¤ºè¯
GENERATE_PROMPT = (
    "ä½ æ˜¯ä¸€ä¸ªé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚"
    "ä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚"
    "å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ã€‚"
    "æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ï¼Œä¿æŒç­”æ¡ˆç®€æ´ã€‚\n"
    "é—®é¢˜ï¼š{question}\n"
    "ä¸Šä¸‹æ–‡ï¼š{context}"
)

def generate_answer(state: MessagesState):
    """
    ç”Ÿæˆç­”æ¡ˆã€‚
    
    å‚æ•°:
        state: åŒ…å«æ¶ˆæ¯å†å²çš„çŠ¶æ€å¯¹è±¡
        
    è¿”å›:
        åŒ…å«ç”Ÿæˆç­”æ¡ˆçš„å­—å…¸
    """
    # è·å–åŸå§‹é—®é¢˜å’Œæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
    question = state["messages"][0].content
    context = state["messages"][-1].content
    
    # æ„å»ºç”Ÿæˆæç¤º
    prompt = GENERATE_PROMPT.format(question=question, context=context)
    
    # è°ƒç”¨æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
    response = response_model.invoke([{"role": "user", "content": prompt}])
    
    return {"messages": [response]}


# ============ ç¬¬ä¸ƒæ­¥ï¼šç»„è£…å›¾ ============
# å°†æ‰€æœ‰èŠ‚ç‚¹å’Œè¾¹è¿æ¥èµ·æ¥å½¢æˆå®Œæ•´çš„å·¥ä½œæµ

print("ğŸ”¨ æ­£åœ¨æ„å»º Agentic RAG å›¾...")

# åˆ›å»ºçŠ¶æ€å›¾
workflow = StateGraph(MessagesState)

# æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
workflow.add_node("generate_query_or_respond", generate_query_or_respond)  # å†³ç­–èŠ‚ç‚¹
workflow.add_node("retrieve", ToolNode([retriever_tool]))  # æ£€ç´¢èŠ‚ç‚¹
workflow.add_node("rewrite_question", rewrite_question)  # é‡å†™é—®é¢˜èŠ‚ç‚¹
workflow.add_node("generate_answer", generate_answer)  # ç”Ÿæˆç­”æ¡ˆèŠ‚ç‚¹

# è®¾ç½®èµ·å§‹è¾¹
workflow.add_edge(START, "generate_query_or_respond")

# æ·»åŠ æ¡ä»¶è¾¹ï¼šå†³å®šæ˜¯å¦éœ€è¦æ£€ç´¢
workflow.add_conditional_edges(
    "generate_query_or_respond",
    tools_condition,  # è¯„ä¼° LLM å†³ç­–ï¼ˆè°ƒç”¨æ£€ç´¢å·¥å…·æˆ–ç›´æ¥å“åº”ç”¨æˆ·ï¼‰
    {
        "tools": "retrieve",  # å¦‚æœéœ€è¦æ£€ç´¢ï¼Œè·³è½¬åˆ°æ£€ç´¢èŠ‚ç‚¹
        END: END,  # å¦‚æœä¸éœ€è¦æ£€ç´¢ï¼Œç›´æ¥ç»“æŸ
    },
)

# æ·»åŠ æ¡ä»¶è¾¹ï¼šè¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡æ¡£
workflow.add_conditional_edges(
    "retrieve",
    grade_documents,  # è¯„ä¼°æ–‡æ¡£ç›¸å…³æ€§
    # æ ¹æ®è¯„åˆ†ç»“æœè·³è½¬åˆ°ä¸åŒèŠ‚ç‚¹
)

# æ·»åŠ å›ºå®šè¾¹
workflow.add_edge("generate_answer", END)  # ç”Ÿæˆç­”æ¡ˆåç»“æŸ
workflow.add_edge("rewrite_question", "generate_query_or_respond")  # é‡å†™é—®é¢˜åé‡æ–°å¼€å§‹

# ç¼–è¯‘å›¾
graph = workflow.compile()

print("âœ… Agentic RAG å›¾æ„å»ºå®Œæˆï¼")


# ============ LangGraph API å·¥å‚å‡½æ•° ============

def get_graph():
    """
    å·¥å‚å‡½æ•° - è¿”å› Agentic RAG Agent Graph

    ä¾› LangGraph API ä½¿ç”¨

    Returns:
        ç¼–è¯‘å¥½çš„ Agentic RAG Agent Graph
    """
    return graph


# ============ ç¬¬å…«æ­¥ï¼šè¿è¡Œ Agentic RAG ============
# æµ‹è¯•ç³»ç»Ÿ

if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("ğŸš€ å¼€å§‹æµ‹è¯• Agentic RAG ç³»ç»Ÿ")
    print("=" * 60 + "\n")
    
    # æµ‹è¯•é—®é¢˜ 1ï¼šéœ€è¦æ£€ç´¢çš„é—®é¢˜
    query1 = "ä»€ä¹ˆæ˜¯ LLM Agentï¼Ÿå®ƒæœ‰å“ªäº›å…³é”®ç»„ä»¶ï¼Ÿ"
    
    print(f"ğŸ“ é—®é¢˜ 1: {query1}\n")
    
    # è¿è¡Œå›¾
    for event in graph.stream(
        {"messages": [{"role": "user", "content": query1}]},
        stream_mode="values",
    ):
        # æ‰“å°æœ€åä¸€æ¡æ¶ˆæ¯
        event["messages"][-1].pretty_print()
        print("-" * 60)
    
    print("\n" + "=" * 60)
    
    # æµ‹è¯•é—®é¢˜ 2ï¼šä¸éœ€è¦æ£€ç´¢çš„ç®€å•é—®é¢˜
    query2 = "ä½ å¥½ï¼"
    
    print(f"ğŸ“ é—®é¢˜ 2: {query2}\n")
    
    # è¿è¡Œå›¾
    for event in graph.stream(
        {"messages": [{"role": "user", "content": query2}]},
        stream_mode="values",
    ):
        # æ‰“å°æœ€åä¸€æ¡æ¶ˆæ¯
        event["messages"][-1].pretty_print()
        print("-" * 60)
    
    print("\n" + "=" * 60)
    print("âœ¨ æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)
